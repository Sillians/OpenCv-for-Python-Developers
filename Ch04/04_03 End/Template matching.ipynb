{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to template matching\n",
    "\n",
    "\n",
    "When it comes to feature detection, template matching is a readily available and straightforward method. The way template matching works, is it searches for a similar pattern between two images. This is accomplished by taking a reference image, called a template and sliding it around the other comparison image, taking a difference at every position. The result, is a black and white gray scale image with varying intensities showing how well it matched at each position. Using a 1D, somewhat modified example, we can see that as we slide our template, which is a triangle, across the screen, it does a comparison in each spot. On the far left there are no matches, so it has a perfectly black value, as it goes to the right and starts to overlap it gets a gray value, indicating a partial match at some of the pixels of the template itself. Then when it perfectly over laps the red triangle in the image, it gives a perfectly white value. As it continues unto the right, it will go back to black indicating that there was a no match. Typically, template matching is actually applied in a two-dimensional format but the concept is the same, your source template image will scroll horizontally and vertically across the entire image, taking a difference at each location. The sum result of that difference is put into the pixel value, where a zero sum difference mean the exact same images becomes white and a perfect difference become black. Typically you'll find there's lots of gray in your image as there are always going to be partial matches of some of the pixels in the template versus your image. The example shown here, we have a yellow ball used as the template. Of course it's only one yellow ball in the actual scene and we can see very clearly in the output that the brightest spot of the image is exactly where we expect the ball to be. However there are partial matches elsewhere in the image most likely where the yellow channels seem to overlap. Typically with template matching you don't actually use an element from the source image yourself but something that is predetermined, such as a face or a known generic object that is expected to be found in the scene. Given that, it's important to understand a few of the limitations of template matching. If your template is scaled compared to your Source image then it will not work very well, likewise if your template is rotated and the template looks different at those different rotations, it may reduce the effectiveness of the template matching. Despite that it's still a very efficient algorithm and can be very useful in some scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Template matching\n",
    "\n",
    "* Now we can dive into template matching as a means to detect features in an image. As you may recall, template matching works by sliding a source template image and making a difference at every possible location against a reference image. This difference shows us how close those two images are together and if there are any close matches between the template and the source frame it will indicate that area as a very bright spot in the resulting image. Note that we have read in both images, here, as zero, meaning they are in grayscale. In this case, we resolve any differences between the color of our original template and the reference image. Next, let's actually display these two by typing cv2 dot imgshow and then we'll type in frame comma frame enter, cv2 dot imgshow template comma template. Now, let's actually run our template matching command. We'll type result equals cv2 dot match template and then we'll pass in the frame followed by the template, followed by the cv2 dot and then in all caps, T-M, underscore, C-C-O-E-F-F, underscore, N-O-R-M-E-D. Note that this is one of several different methods of doing template matching. I recommend looking at the OpenCV documentation for all the other options. At this point, we can already take a look at a result by typing cv2 dot imgshow and then, we'll pass in the name of the window called matching comma result. And finally, we'll do cv2 dot waitKey, with a K as capitalized, pass in zero, cv2 dot destroy all windows. Pressing Save, I'm going to go to my command-line prompt, make sure that I'm in the right location and that my files exist and then I will type python3 04, underscore, 03 dot py. Upon pressing enter, I can now see the result of our template matching. we can see how this difference compares the source image to the template. Again, if I were to quote, unquote, animate the template working it would work by moving this image across the source frame, like so, at every possible location One thing to note is that the size of the matching frame is actually less than the size of the original frame. This is because if we were to overlay the two you would note that the template file can only match in the top, left-hand corner of the original frame, which means the first pixel of the difference value is actually at the center of the template at that point. In the same way, the last pixel does a difference at the bottom, right-hand corner of the matching result isn't quite the bottom right-hand corner of the actual frame As we can see from the matching window, there's a varying degree of brightness across the image but we note that the area that seems to have the brightest from a visual standpoint is around here, which, if we move the matching window out of the way, we can see that is the approximate location of our original ball. Even though we're using a different ball as our template. Now let's see if we can more exactly determine what that location is. I'm going to press Escape to exit out of the window and then I'll return to my scripting window. What we're going to do now is try and draw that location by getting the maximum brightness out of the result image. We can do this by making use of the binmax function. Typing in front of the imgshow, I'm going to write min, underscore, val, comma, max, underscore, val, comma, min, underscore, loc, comma, max, underscore, loc, equals, cv2, dot, min max, where the M is capitalized, loc, where the L is capitalized, then I'll pass in result. Now, we can print the maximum value, comma, max, underscore, loc and then, if we want to try and actually visualize where this location is, we can type cv2 dot circle and then pass in our resulting image, result, comma, use the max, underscore, loc, give in whatever radius we like, such as 15, for example. Give it a color of pure white, noting that we're in a grayscale image, so we only have to pass in one value and a line thickness of two. At this point, if I press Save and return to my console, press up arrow one time and press Enter, we can see that it successfully found that that point there, that has a circle, is the brightest point, which would, in fact, match our location of the soccer ball in the original frame. Likewise, if you go back to the console, we can see that it printed out the value of the match at that location and it's x/y location. We can see that it was not quite a perfect match but it was still the brightest match in the image. This demonstrates that, while template matching can be a useful method, it's certainly not perfect and we can see how any variances between our source template may not result in a perfect match. If we had, for example, used the original soccer ball from the image itself as our template, we would find that the result matching value, here, would have been much closer to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = cv2.imread('template.jpg',0)\n",
    "frame = cv2.imread(\"players.jpg\",0)\n",
    "\n",
    "cv2.imshow(\"Frame\",frame)\n",
    "cv2.imshow(\"Template\",template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46597495675086975 (132, 243)\n"
     ]
    }
   ],
   "source": [
    "result = cv2.matchTemplate(frame, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "print(max_val,max_loc)\n",
    "cv2.circle(result,max_loc, 15,255,2)\n",
    "cv2.imshow(\"Matching\",result)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
